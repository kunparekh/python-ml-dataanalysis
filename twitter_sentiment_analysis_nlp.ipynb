{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - NLP\n",
    "Content:\\\n",
    "target: the polarity of the tweet (0 = negative, 4 = positive)\\\n",
    "ids: The id of the tweet ( 2087)\\\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\\\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\\\n",
    "user: the user that tweeted (robotickilldozr)\\\n",
    "text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "dataset: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kparekh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import re, time\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448282</th>\n",
       "      <td>0</td>\n",
       "      <td>2068921155</td>\n",
       "      <td>Sun Jun 07 14:56:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>smiley_sophie</td>\n",
       "      <td>my arm still hurts from when i pulled it yeste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475261</th>\n",
       "      <td>4</td>\n",
       "      <td>2065871668</td>\n",
       "      <td>Sun Jun 07 09:27:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ImmaChocoholic</td>\n",
       "      <td>I have so much to do outside! Been looking at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132529</th>\n",
       "      <td>0</td>\n",
       "      <td>1835774749</td>\n",
       "      <td>Mon May 18 06:43:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>drmomentum</td>\n",
       "      <td>@AbsolutSara Yes, I knew about the clusterfark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182348</th>\n",
       "      <td>0</td>\n",
       "      <td>1967121891</td>\n",
       "      <td>Fri May 29 19:00:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sweetsheilx</td>\n",
       "      <td>Just woke up and i feel relieved Haha now i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907614</th>\n",
       "      <td>4</td>\n",
       "      <td>1695846172</td>\n",
       "      <td>Mon May 04 07:04:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>monmariej</td>\n",
       "      <td>LOVING the hot weather forecast for the rest o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "448282        0  2068921155  Sun Jun 07 14:56:42 PDT 2009  NO_QUERY   \n",
       "1475261       4  2065871668  Sun Jun 07 09:27:21 PDT 2009  NO_QUERY   \n",
       "132529        0  1835774749  Mon May 18 06:43:27 PDT 2009  NO_QUERY   \n",
       "182348        0  1967121891  Fri May 29 19:00:46 PDT 2009  NO_QUERY   \n",
       "907614        4  1695846172  Mon May 04 07:04:29 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "448282    smiley_sophie  my arm still hurts from when i pulled it yeste...  \n",
       "1475261  ImmaChocoholic  I have so much to do outside! Been looking at ...  \n",
       "132529       drmomentum  @AbsolutSara Yes, I knew about the clusterfark...  \n",
       "182348      sweetsheilx  Just woke up and i feel relieved Haha now i ha...  \n",
       "907614        monmariej  LOVING the hot weather forecast for the rest o...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "data = pd.read_csv('C:/Users/kparekh/Downloads/training.1600000.processed.noemoticon.csv', header= None, names=['target', 'ids', 'date', 'flag', 'user', 'text'], encoding='latin-1')\n",
    "data = data.sample(n=100000, random_state = 123)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop columns we don't need\n",
    "data = data.drop(['ids', 'date', 'flag', 'user'], 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.0\n",
       "mean         74.0\n",
       "std          36.0\n",
       "min           7.0\n",
       "25%          44.0\n",
       "50%          69.0\n",
       "75%         104.0\n",
       "max         270.0\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = data.text.str.len()\n",
    "data.length.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448282    my arm still hurts from when i pulled it yeste...\n",
       "132529    @AbsolutSara Yes, I knew about the clusterfark...\n",
       "182348    Just woke up and i feel relieved Haha now i ha...\n",
       "12387     morning folks... here i am bored at work again...\n",
       "279890    @princessofmars I am lost. Please help me find...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.target==0].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1475261    I have so much to do outside! Been looking at ...\n",
       "907614     LOVING the hot weather forecast for the rest o...\n",
       "1338189    Having a productive morning - then a 90-minute...\n",
       "926369     @JimFoss Love it, used to do the same thing at...\n",
       "1302919    leaving the nest while i prepare din-din. twee...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.target==4].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.50078\n",
       "4    0.49922\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def preprocess(text):\n",
    "    text = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', str(text).lower().strip())\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text_cleaned'] = data.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 80000\n",
      "Test size: 20000\n"
     ]
    }
   ],
   "source": [
    "#split data into train and test\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "print(\"Train size:\", len(data_train))\n",
    "print(\"Test size:\", len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have enough volume of data, we can attempt a neural network with word2vec embeddings - words that are related to each other are mapped to points that are closer to each other in a high dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train word2vec embeddings\n",
    "text_tokens = [text.split() for text in data_train.text_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 9217\n",
      "Wall time: 15min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = gensim.models.Word2Vec(text_tokens, size=100, window = 5, seed = 1, min_count = 5, workers = 8)\n",
    "words = w2v_model.wv.vocab.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kparekh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('awesome', 0.8805774450302124),\n",
       " ('plz', 0.8576584458351135),\n",
       " ('rock', 0.8539444208145142),\n",
       " ('miley', 0.8483649492263794),\n",
       " ('cute', 0.8483544588088989),\n",
       " ('amazing', 0.8472157716751099),\n",
       " ('luv', 0.8441596031188965),\n",
       " ('tom', 0.8394282460212708),\n",
       " ('loved', 0.8328625559806824),\n",
       " ('w8', 0.832410454750061)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 50405\n"
     ]
    }
   ],
   "source": [
    "#tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_train.text_cleaned)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences to a specified length\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(data_train.text), maxlen=300)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(data_test.text), maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (80000, 300)\n",
      "y_train (80000, 1)\n",
      "x_test (20000, 300)\n",
      "y_test (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "#encode target\n",
    "labels = data_train.target.unique().tolist()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_train.target.tolist())\n",
    "\n",
    "y_train = encoder.transform(data_train.target.tolist())\n",
    "y_test = encoder.transform(data_test.target.tolist())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50405, 100)\n"
     ]
    }
   ],
   "source": [
    "#prepare an embedding layer to pass into the neural network\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in w2v_model.wv:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=300, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 100)          5040500   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,121,001\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 5,040,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks for learning rate and early stopping\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1\n",
      "72000/72000 [==============================] - 620s 9ms/step - loss: 0.6278 - acc: 0.6437 - val_loss: 0.5978 - val_acc: 0.6759\n"
     ]
    }
   ],
   "source": [
    "#fit model - for now we will only train on one epoch/ the model should be trained for longer for optimal performance\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=1,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 33s 2ms/step\n",
      "\n",
      "ACCURACY: 0.66905\n",
      "LOSS: 0.598604208946228\n"
     ]
    }
   ],
   "source": [
    "#evaluate performance on test set\n",
    "score = model.evaluate(x_test, y_test, batch_size=512)\n",
    "print()\n",
    "print(\"ACCURACY:\",score[1])\n",
    "print(\"LOSS:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = 'NEUTRAL'\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = 'NEGATIVE'\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = 'POSITIVE'\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return 'NEGATIVE' if score < 0.5 else 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, include_neutral=True):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=300)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=True)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE',\n",
       " 'score': 0.7171138525009155,\n",
       " 'elapsed_time': 0.03290843963623047}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I love cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'NEGATIVE',\n",
       " 'score': 0.3009055256843567,\n",
       " 'elapsed_time': 0.026886701583862305}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I don't know anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above performance shows pretty good results with little data compared to what's required for neural networks. The performance can definetely be improved with more data, longer training, and optimized hyperparameters.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
